{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of 2019/10/3 (student) PM25＋三個練習題.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "JjpVj8Ne_p6R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZX_HQWt68dR",
        "colab_type": "code",
        "outputId": "64bbdbb8-ce24-4fe3-a635-bff2d89f51da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# 環境初始化 (大約三至五分鐘)\n",
        "! wget -O init_env.sh https://www.dropbox.com/s/6bnwn8u2hz19s59/init_env.sh && \\\n",
        "bash init_env.sh"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-10-27 08:12:07--  https://www.dropbox.com/s/6bnwn8u2hz19s59/init_env.sh\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.1.1, 2620:100:601b:1::a27d:801\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.1.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/6bnwn8u2hz19s59/init_env.sh [following]\n",
            "--2019-10-27 08:12:12--  https://www.dropbox.com/s/raw/6bnwn8u2hz19s59/init_env.sh\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ucaba27e9a390287a1f95bf7f2bf.dl.dropboxusercontent.com/cd/0/inline/ArPdheCNuP3A1ag2yEpa4ORJUunzRiT6gj-Bu1tVPrKigFAboA_x-Jx9qikq-dEZQNbdL4ivqTT8VUn-TwRaLeyGy6RKDZH3fRE4NxBMWVb7Zg/file# [following]\n",
            "--2019-10-27 08:12:12--  https://ucaba27e9a390287a1f95bf7f2bf.dl.dropboxusercontent.com/cd/0/inline/ArPdheCNuP3A1ag2yEpa4ORJUunzRiT6gj-Bu1tVPrKigFAboA_x-Jx9qikq-dEZQNbdL4ivqTT8VUn-TwRaLeyGy6RKDZH3fRE4NxBMWVb7Zg/file\n",
            "Resolving ucaba27e9a390287a1f95bf7f2bf.dl.dropboxusercontent.com (ucaba27e9a390287a1f95bf7f2bf.dl.dropboxusercontent.com)... 162.125.1.6, 2620:100:601b:6::a27d:806\n",
            "Connecting to ucaba27e9a390287a1f95bf7f2bf.dl.dropboxusercontent.com (ucaba27e9a390287a1f95bf7f2bf.dl.dropboxusercontent.com)|162.125.1.6|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 336 [text/plain]\n",
            "Saving to: ‘init_env.sh’\n",
            "\n",
            "init_env.sh         100%[===================>]     336  --.-KB/s    in 0s      \n",
            "\n",
            "2019-10-27 08:12:13 (55.8 MB/s) - ‘init_env.sh’ saved [336/336]\n",
            "\n",
            "--2019-10-27 08:12:13--  https://d3kbcqa49mib13.cloudfront.net/spark-2.2.0-bin-hadoop2.7.tgz\n",
            "Resolving d3kbcqa49mib13.cloudfront.net (d3kbcqa49mib13.cloudfront.net)... 99.86.32.10, 99.86.32.227, 99.86.32.112, ...\n",
            "Connecting to d3kbcqa49mib13.cloudfront.net (d3kbcqa49mib13.cloudfront.net)|99.86.32.10|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 203728858 (194M) [application/x-tar]\n",
            "Saving to: ‘spark-2.2.0-bin-hadoop2.7.tgz’\n",
            "\n",
            "spark-2.2.0-bin-had 100%[===================>] 194.29M   153MB/s    in 1.3s    \n",
            "\n",
            "2019-10-27 08:12:14 (153 MB/s) - ‘spark-2.2.0-bin-hadoop2.7.tgz’ saved [203728858/203728858]\n",
            "\n",
            "spark-2.2.0-bin-hadoop2.7/\n",
            "spark-2.2.0-bin-hadoop2.7/NOTICE\n",
            "spark-2.2.0-bin-hadoop2.7/jars/\n",
            "spark-2.2.0-bin-hadoop2.7/jars/parquet-common-1.8.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/bonecp-0.8.0.RELEASE.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/commons-net-2.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/javax.servlet-api-3.1.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/hadoop-annotations-2.7.3.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/hadoop-hdfs-2.7.3.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/oro-2.0.8.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/xercesImpl-2.9.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/antlr-runtime-3.4.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/machinist_2.11-0.6.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/spire_2.11-0.13.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/parquet-hadoop-1.8.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/spark-sketch_2.11-2.2.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/stream-2.7.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/kryo-shaded-3.0.3.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/metrics-jvm-3.1.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/hadoop-mapreduce-client-common-2.7.3.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/RoaringBitmap-0.5.11.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/hadoop-auth-2.7.3.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/hadoop-common-2.7.3.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/activation-1.1.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jta-1.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/datanucleus-core-3.2.10.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jersey-container-servlet-2.22.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jersey-guava-2.22.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jets3t-0.9.3.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jetty-util-6.1.26.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/commons-compress-1.4.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jersey-container-servlet-core-2.22.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/commons-beanutils-1.7.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/hk2-utils-2.4.0-b34.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/parquet-format-2.3.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/avro-1.7.7.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/datanucleus-api-jdo-3.2.6.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jline-2.12.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/metrics-core-3.1.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/java-xmlbuilder-1.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/stax-api-1.0-2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/hk2-locator-2.4.0-b34.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/parquet-hadoop-bundle-1.6.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jsp-api-2.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/xmlenc-0.52.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/xbean-asm5-shaded-4.4.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jackson-core-asl-1.9.13.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/shapeless_2.11-2.3.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/commons-collections-3.2.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/javax.inject-1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/spark-sql_2.11-2.2.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/json4s-jackson_2.11-3.2.11.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/json4s-ast_2.11-3.2.11.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/commons-codec-1.10.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/leveldbjni-all-1.8.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/commons-httpclient-3.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/aopalliance-repackaged-2.4.0-b34.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/hadoop-yarn-server-common-2.7.3.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/minlog-1.3.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/javolution-5.5.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/datanucleus-rdbms-3.2.9.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jersey-common-2.22.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/spark-graphx_2.11-2.2.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/api-asn1-api-1.0.0-M20.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/apacheds-i18n-2.0.0-M15.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/validation-api-1.1.0.Final.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/commons-dbcp-1.4.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/slf4j-log4j12-1.7.16.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/commons-pool-1.5.4.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/spark-network-common_2.11-2.2.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/pmml-schema-1.2.15.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/hadoop-mapreduce-client-shuffle-2.7.3.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/joda-time-2.9.3.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/spark-hive-thriftserver_2.11-2.2.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/spire-macros_2.11-0.13.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/curator-recipes-2.6.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jersey-server-2.22.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/htrace-core-3.1.0-incubating.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/httpclient-4.5.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/spark-mllib-local_2.11-2.2.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/snappy-0.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/breeze-macros_2.11-0.13.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jackson-jaxrs-1.9.13.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/eigenbase-properties-1.1.5.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/spark-tags_2.11-2.2.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jackson-databind-2.6.5.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/curator-client-2.6.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/spark-catalyst_2.11-2.2.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/paranamer-2.6.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/opencsv-2.3.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/json4s-core_2.11-3.2.11.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/hive-metastore-1.2.1.spark2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/commons-digester-1.8.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jsr305-1.3.9.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/spark-repl_2.11-2.2.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jetty-6.1.26.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/pyrolite-4.13.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/log4j-1.2.17.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/hadoop-yarn-server-web-proxy-2.7.3.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/calcite-avatica-1.2.0-incubating.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/scala-xml_2.11-1.0.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/spark-network-shuffle_2.11-2.2.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/calcite-linq4j-1.2.0-incubating.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/compress-lzf-1.0.3.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jtransforms-2.4.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/apache-log4j-extras-1.2.17.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jcl-over-slf4j-1.7.16.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/guice-3.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/gson-2.2.4.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/httpcore-4.4.4.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/protobuf-java-2.5.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/hadoop-mapreduce-client-core-2.7.3.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/spark-yarn_2.11-2.2.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/spark-hive_2.11-2.2.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/commons-lang-2.6.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/stax-api-1.0.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/javax.annotation-api-1.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/netty-all-4.0.43.Final.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/curator-framework-2.6.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/commons-io-2.4.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/avro-ipc-1.7.7.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/hive-beeline-1.2.1.spark2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/mesos-1.0.0-shaded-protobuf.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jackson-annotations-2.6.5.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jersey-media-jaxb-2.22.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/hadoop-yarn-common-2.7.3.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jackson-mapper-asl-1.9.13.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/parquet-jackson-1.8.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/hadoop-client-2.7.3.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jackson-module-paranamer-2.6.5.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/aopalliance-1.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/super-csv-2.2.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/janino-3.0.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/antlr4-runtime-4.5.3.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jpam-1.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/commons-lang3-3.5.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/javassist-3.18.1-GA.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/bcprov-jdk15on-1.51.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/spark-launcher_2.11-2.2.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/javax.ws.rs-api-2.0.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/hadoop-mapreduce-client-jobclient-2.7.3.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/metrics-graphite-3.1.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jackson-xc-1.9.13.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/lz4-1.3.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/core-1.1.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/spark-mesos_2.11-2.2.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/antlr-2.7.7.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/hadoop-yarn-client-2.7.3.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/mx4j-3.0.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/commons-logging-1.1.3.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/commons-beanutils-core-1.8.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/libfb303-0.9.3.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/libthrift-0.9.3.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jaxb-api-2.2.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/hive-exec-1.2.1.spark2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/calcite-core-1.2.0-incubating.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/parquet-encoding-1.8.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/spark-mllib_2.11-2.2.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/api-util-1.0.0-M20.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/apacheds-kerberos-codec-2.0.0-M15.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/mail-1.4.7.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/stringtemplate-3.2.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/guice-servlet-3.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/commons-crypto-1.0.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/JavaEWAH-0.3.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/metrics-json-3.1.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jdo-api-3.0.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/scalap-2.11.8.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/hive-cli-1.2.1.spark2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/zookeeper-3.4.6.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jackson-module-scala_2.11-2.6.5.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/ivy-2.4.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/py4j-0.10.4.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/arpack_combined_all-0.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/spark-unsafe_2.11-2.2.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/macro-compat_2.11-1.1.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jul-to-slf4j-1.7.16.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/pmml-model-1.2.15.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/parquet-column-1.8.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/javax.inject-2.4.0-b34.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/breeze_2.11-0.13.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/commons-cli-1.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/chill-java-0.8.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/avro-mapred-1.7.7-hadoop2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/snappy-java-1.1.2.6.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/base64-2.3.8.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/commons-compiler-3.0.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/slf4j-api-1.7.16.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/derby-10.12.1.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/hadoop-mapreduce-client-app-2.7.3.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/objenesis-2.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jodd-core-3.5.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jersey-client-2.22.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/ST4-4.0.4.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/univocity-parsers-2.2.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/scala-parser-combinators_2.11-1.0.4.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jackson-core-2.6.5.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/commons-math3-3.4.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/xz-1.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/scala-reflect-2.11.8.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/commons-configuration-1.6.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/scala-compiler-2.11.8.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/hadoop-yarn-api-2.7.3.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/hk2-api-2.4.0-b34.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/spark-core_2.11-2.2.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/guava-14.0.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/netty-3.9.9.Final.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/spark-streaming_2.11-2.2.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/chill_2.11-0.8.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/scala-library-2.11.8.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/osgi-resource-locator-1.0.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/python/\n",
            "spark-2.2.0-bin-hadoop2.7/python/run-tests.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/hello/\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/hello/sub_hello/\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/hello/sub_hello/sub_hello.txt\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/hello/hello.txt\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/userlibrary.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/userlib-0.1.zip\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/people.json\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/people1.json\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/orc_partitioned/\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=1/\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=1/c=1/\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=1/c=1/part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=1/c=1/.part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc.crc\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/orc_partitioned/_SUCCESS\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=0/\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=0/c=0/\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=0/c=0/part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=0/c=0/.part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc.crc\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/streaming/\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/streaming/text-test.txt\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/text-test.txt\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/people_array.json\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2014/\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2014/month=9/\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2014/month=9/day=1/\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2014/month=9/day=1/.part-r-00008.gz.parquet.crc\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2014/month=9/day=1/part-r-00008.gz.parquet\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/_SUCCESS\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=26/\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=26/part-r-00005.gz.parquet\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=26/.part-r-00005.gz.parquet.crc\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/part-r-00002.gz.parquet\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/.part-r-00002.gz.parquet.crc\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/part-r-00004.gz.parquet\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/.part-r-00004.gz.parquet.crc\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=9/\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=9/day=1/\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=9/day=1/.part-r-00007.gz.parquet.crc\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=9/day=1/part-r-00007.gz.parquet\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/_metadata\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/_common_metadata\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/ages_newlines.csv\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/ages.csv\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/SimpleHTTPServer.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pylintrc\n",
            "spark-2.2.0-bin-hadoop2.7/python/docs/\n",
            "spark-2.2.0-bin-hadoop2.7/python/docs/pyspark.ml.rst\n",
            "spark-2.2.0-bin-hadoop2.7/python/docs/pyspark.streaming.rst\n",
            "spark-2.2.0-bin-hadoop2.7/python/docs/conf.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/docs/_templates/\n",
            "spark-2.2.0-bin-hadoop2.7/python/docs/_templates/layout.html\n",
            "spark-2.2.0-bin-hadoop2.7/python/docs/pyspark.rst\n",
            "spark-2.2.0-bin-hadoop2.7/python/docs/make.bat\n",
            "spark-2.2.0-bin-hadoop2.7/python/docs/epytext.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/docs/make2.bat\n",
            "spark-2.2.0-bin-hadoop2.7/python/docs/index.rst\n",
            "spark-2.2.0-bin-hadoop2.7/python/docs/_static/\n",
            "spark-2.2.0-bin-hadoop2.7/python/docs/_static/pyspark.js\n",
            "spark-2.2.0-bin-hadoop2.7/python/docs/_static/pyspark.css\n",
            "spark-2.2.0-bin-hadoop2.7/python/docs/pyspark.sql.rst\n",
            "spark-2.2.0-bin-hadoop2.7/python/docs/pyspark.mllib.rst\n",
            "spark-2.2.0-bin-hadoop2.7/python/docs/Makefile\n",
            "spark-2.2.0-bin-hadoop2.7/python/.gitignore\n",
            "spark-2.2.0-bin-hadoop2.7/python/MANIFEST.in\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/status.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/version.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/conf.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/base.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/evaluation.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/util.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/classification.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/regression.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/tests.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/tuning.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/common.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/stat.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/linalg/\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/linalg/__init__.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/pipeline.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/feature.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/clustering.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/recommendation.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/__init__.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/wrapper.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/param/\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/param/_shared_params_code_gen.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/param/shared.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/param/__init__.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/fpm.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/statcounter.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/profiler.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/serializers.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/traceback_utils.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/shell.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/conf.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/session.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/window.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/tests.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/utils.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/group.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/types.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/catalog.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/context.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/dataframe.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/column.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/streaming.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/__init__.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/readwriter.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/functions.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/taskcontext.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/util.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/python/\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/python/pyspark/\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/python/pyspark/shell.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/daemon.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/tests.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/resultiterable.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/heapq3.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/broadcast.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/shuffle.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/cloudpickle.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/accumulators.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/java_gateway.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/streaming/\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/streaming/util.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/streaming/listener.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/streaming/tests.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/streaming/flume.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/streaming/kafka.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/streaming/kinesis.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/streaming/dstream.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/streaming/context.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/streaming/__init__.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/context.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/storagelevel.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/__init__.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/join.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/tree.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/evaluation.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/util.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/classification.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/regression.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/tests.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/common.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/linalg/\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/linalg/__init__.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/linalg/distributed.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/feature.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/clustering.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/recommendation.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/stat/\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/stat/__init__.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/stat/_statistics.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/stat/KernelDensity.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/stat/test.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/stat/distribution.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/random.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/__init__.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/fpm.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/rdd.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/rddsampler.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/worker.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/files.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/find_spark_home.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/setup.cfg\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark.egg-info/\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark.egg-info/SOURCES.txt\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark.egg-info/requires.txt\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark.egg-info/PKG-INFO\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark.egg-info/dependency_links.txt\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark.egg-info/top_level.txt\n",
            "spark-2.2.0-bin-hadoop2.7/python/run-tests\n",
            "spark-2.2.0-bin-hadoop2.7/python/dist/\n",
            "spark-2.2.0-bin-hadoop2.7/python/setup.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/lib/\n",
            "spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip\n",
            "spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip\n",
            "spark-2.2.0-bin-hadoop2.7/python/lib/PY4J_LICENSE.txt\n",
            "spark-2.2.0-bin-hadoop2.7/python/README.md\n",
            "spark-2.2.0-bin-hadoop2.7/RELEASE\n",
            "spark-2.2.0-bin-hadoop2.7/sbin/\n",
            "spark-2.2.0-bin-hadoop2.7/sbin/start-mesos-shuffle-service.sh\n",
            "spark-2.2.0-bin-hadoop2.7/sbin/start-mesos-dispatcher.sh\n",
            "spark-2.2.0-bin-hadoop2.7/sbin/spark-daemon.sh\n",
            "spark-2.2.0-bin-hadoop2.7/sbin/stop-slaves.sh\n",
            "spark-2.2.0-bin-hadoop2.7/sbin/stop-thriftserver.sh\n",
            "spark-2.2.0-bin-hadoop2.7/sbin/stop-shuffle-service.sh\n",
            "spark-2.2.0-bin-hadoop2.7/sbin/stop-history-server.sh\n",
            "spark-2.2.0-bin-hadoop2.7/sbin/spark-config.sh\n",
            "spark-2.2.0-bin-hadoop2.7/sbin/start-history-server.sh\n",
            "spark-2.2.0-bin-hadoop2.7/sbin/start-thriftserver.sh\n",
            "spark-2.2.0-bin-hadoop2.7/sbin/start-shuffle-service.sh\n",
            "spark-2.2.0-bin-hadoop2.7/sbin/spark-daemons.sh\n",
            "spark-2.2.0-bin-hadoop2.7/sbin/start-all.sh\n",
            "spark-2.2.0-bin-hadoop2.7/sbin/stop-master.sh\n",
            "spark-2.2.0-bin-hadoop2.7/sbin/stop-mesos-dispatcher.sh\n",
            "spark-2.2.0-bin-hadoop2.7/sbin/stop-slave.sh\n",
            "spark-2.2.0-bin-hadoop2.7/sbin/start-slave.sh\n",
            "spark-2.2.0-bin-hadoop2.7/sbin/stop-mesos-shuffle-service.sh\n",
            "spark-2.2.0-bin-hadoop2.7/sbin/start-slaves.sh\n",
            "spark-2.2.0-bin-hadoop2.7/sbin/stop-all.sh\n",
            "spark-2.2.0-bin-hadoop2.7/sbin/slaves.sh\n",
            "spark-2.2.0-bin-hadoop2.7/sbin/start-master.sh\n",
            "spark-2.2.0-bin-hadoop2.7/examples/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/r/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/r/RSparkSQLExample.R\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/fpm.R\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/gbt.R\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/isoreg.R\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/randomForest.R\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/als.R\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/kstest.R\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/gaussianMixture.R\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/ml.R\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/survreg.R\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/glm.R\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/kmeans.R\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/naiveBayes.R\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/svmLinear.R\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/lda.R\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/bisectingKmeans.R\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/mlp.R\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/logit.R\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/r/data-manipulation.R\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/r/dataframe.R\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/r/streaming/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/r/streaming/structured_network_wordcount.R\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/status_api_demo.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/linearsvc.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/random_forest_regressor_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/stopwords_remover_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/min_hash_lsh_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/standard_scaler_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/multiclass_logistic_regression_with_elastic_net.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/fpgrowth_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/vector_assembler_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/max_abs_scaler_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/binarizer_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/imputer_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/bisecting_k_means_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/cross_validator.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/logistic_regression_summary_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/dataframe_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/naive_bayes_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/generalized_linear_regression_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/decision_tree_regression_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/count_vectorizer_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/one_vs_rest_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/linear_regression_with_elastic_net.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/index_to_string_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/sql_transformer.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/correlation_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/dct_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/gradient_boosted_tree_regressor_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/isotonic_regression_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/vector_slicer_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/multilayer_perceptron_classification.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/gradient_boosted_tree_classifier_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/kmeans_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/pca_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/lda_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/polynomial_expansion_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/onehot_encoder_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/aft_survival_regression.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/rformula_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/logistic_regression_with_elastic_net.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/decision_tree_classification_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/estimator_transformer_param_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/tokenizer_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/min_max_scaler_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/pipeline_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/random_forest_classifier_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/string_indexer_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/als_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/n_gram_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/vector_indexer_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/normalizer_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/quantile_discretizer_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/train_validation_split.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/chisq_selector_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/gaussian_mixture_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/bucketed_random_projection_lsh_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/bucketizer_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/elementwise_product_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/tf_idf_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/word2vec_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/chi_square_test_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/pagerank.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/sql/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/sql/hive.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/sql/basic.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/sql/datasource.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/sql/streaming/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/sql/streaming/structured_kafka_wordcount.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/sql/streaming/structured_network_wordcount_windowed.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/sql/streaming/structured_network_wordcount.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/wordcount.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/pi.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/logistic_regression.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/streaming/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/streaming/network_wordjoinsentiments.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/streaming/sql_network_wordcount.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/streaming/queue_stream.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/streaming/network_wordcount.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/streaming/kafka_wordcount.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/streaming/stateful_network_wordcount.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/streaming/direct_kafka_wordcount.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/streaming/hdfs_wordcount.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/streaming/recoverable_network_wordcount.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/streaming/flume_wordcount.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/transitive_closure.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/kmeans.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/avro_inputformat.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/linear_regression_with_sgd_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/svd_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/recommendation_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/regression_metrics_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/standard_scaler_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/kernel_density_estimation_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/random_forest_regression_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/sampled_rdds.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/fpgrowth_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/streaming_k_means_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/latent_dirichlet_allocation_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/bisecting_k_means_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/gradient_boosting_classification_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/multi_class_metrics_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/correlations_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/pca_rowmatrix_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/naive_bayes_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/decision_tree_regression_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/ranking_metrics_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/gaussian_mixture_model.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/logistic_regression_with_lbfgs_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/gradient_boosting_regression_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/power_iteration_clustering_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/binary_classification_metrics_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/logistic_regression.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/hypothesis_testing_kolmogorov_smirnov_test_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/isotonic_regression_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/word2vec.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/summary_statistics_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/multi_label_metrics_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/hypothesis_testing_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/kmeans.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/decision_tree_classification_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/random_rdd_generation.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/svm_with_sgd_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/streaming_linear_regression_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/stratified_sampling_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/normalizer_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/random_forest_classification_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/gaussian_mixture_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/elementwise_product_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/tf_idf_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/word2vec_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/correlations.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/k_means_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/parquet_inputformat.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/als.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/sort.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaPCAExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaModelSelectionViaCrossValidationExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaAFTSurvivalRegressionExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaMaxAbsScalerExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLogisticRegressionSummaryExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLogisticRegressionWithElasticNetExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLinearSVCExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorIndexerExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaKMeansExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaPipelineExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaRandomForestRegressorExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaTfIdfExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorAssemblerExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaInteractionExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaBucketizerExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaDecisionTreeRegressionExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaSQLTransformerExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaDocument.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLinearRegressionWithElasticNetExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaBucketedRandomProjectionLSHExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaGeneralizedLinearRegressionExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLDAExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaDCTExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaNaiveBayesExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLabeledDocument.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaIndexToStringExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaImputerExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaOneVsRestExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaEstimatorTransformerParamExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaBisectingKMeansExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaGaussianMixtureExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaFPGrowthExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaStopWordsRemoverExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaCountVectorizerExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaRFormulaExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaOneHotEncoderExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaQuantileDiscretizerExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaRandomForestClassifierExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaCorrelationExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaIsotonicRegressionExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaGradientBoostedTreeClassifierExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaALSExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaMultilayerPerceptronClassifierExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaMulticlassLogisticRegressionWithElasticNetExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaMinHashLSHExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaNGramExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaStandardScalerExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaDecisionTreeClassificationExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaNormalizerExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaPolynomialExpansionExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaBinarizerExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaGradientBoostedTreeRegressorExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaStringIndexerExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaWord2VecExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorSlicerExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaChiSqSelectorExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaMinMaxScalerExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaElementwiseProductExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaTokenizerExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaChiSquareTestExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaModelSelectionViaTrainValidationSplitExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaSparkPi.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/hive/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/hive/JavaSparkHiveExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/JavaUserDefinedTypedAggregation.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredKafkaWordCount.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredNetworkWordCountWindowed.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredNetworkWordCount.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredSessionization.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/JavaUserDefinedUntypedAggregation.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaLogQuery.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaTC.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaStatusTrackerDemo.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaRecord.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaFlumeEventCount.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaDirectKafkaWordCount.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaNetworkWordCount.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaSqlNetworkWordCount.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaRecoverableNetworkWordCount.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaStatefulNetworkWordCount.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaCustomReceiver.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaQueueStream.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaKafkaWordCount.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaHdfsLR.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaPageRank.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaWordCount.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaPCAExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaSVDExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaKMeansExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaLBFGSExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaKernelDensityEstimationExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaMultiLabelClassificationMetricsExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaSVMWithSGDExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaMulticlassClassificationMetricsExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaPrefixSpanExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaLatentDirichletAllocationExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaDecisionTreeRegressionExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaAssociationRulesExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaStreamingTestExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaBinaryClassificationMetricsExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaRandomForestClassificationExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaRandomForestRegressionExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaStratifiedSamplingExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaNaiveBayesExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaGradientBoostingClassificationExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaPowerIterationClusteringExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaRankingMetricsExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaALS.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaRecommendationExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaBisectingKMeansExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaGaussianMixtureExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaGradientBoostingRegressionExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaIsotonicRegressionExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaSimpleFPGrowth.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaRegressionMetricsExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaDecisionTreeClassificationExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaHypothesisTestingExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaCorrelationsExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaHypothesisTestingKolmogorovSmirnovTestExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaLogisticRegressionWithLBFGSExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaChiSqSelectorExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaElementwiseProductExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaSummaryStatisticsExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaLinearRegressionWithSGDExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/DFSReadWriteTest.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/GroupByTest.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LinearSVCExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/RandomForestExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/InteractionExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/MaxAbsScalerExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/SQLTransformerExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/StandardScalerExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/BisectingKMeansExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ElementwiseProductExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/IndexToStringExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LogisticRegressionSummaryExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ALSExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/CorrelationExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/BucketizerExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/VectorIndexerExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/MinHashLSHExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/StringIndexerExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/TokenizerExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LogisticRegressionExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/UnaryTransformerExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/PCAExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/StopWordsRemoverExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/MultilayerPerceptronClassifierExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/IsotonicRegressionExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/GBTExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ImputerExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/RandomForestRegressorExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ChiSquareTestExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DeveloperApiExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ChiSqSelectorExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/TfIdfExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DecisionTreeExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/BucketedRandomProjectionLSHExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/BinarizerExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/GradientBoostedTreeClassifierExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/MinMaxScalerExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/OneVsRestExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/OneHotEncoderExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/VectorAssemblerExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/Word2VecExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/NaiveBayesExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/PipelineExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/RandomForestClassifierExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DCTExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/GradientBoostedTreeRegressorExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LinearRegressionWithElasticNetExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DataFrameExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ModelSelectionViaCrossValidationExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/MulticlassLogisticRegressionWithElasticNetExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DecisionTreeClassificationExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/KMeansExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/EstimatorTransformerParamExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ModelSelectionViaTrainValidationSplitExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/CountVectorizerExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/VectorSlicerExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/NormalizerExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/FPGrowthExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/GaussianMixtureExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/PolynomialExpansionExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LogisticRegressionWithElasticNetExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LinearRegressionExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DecisionTreeRegressionExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LDAExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/GeneralizedLinearRegressionExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/RFormulaExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/AFTSurvivalRegressionExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/QuantileDiscretizerExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/NGramExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkKMeans.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/MultiBroadcastTest.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/hive/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/hive/SparkHiveExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/SparkSQLExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/UserDefinedUntypedAggregation.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/RDDRelation.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/UserDefinedTypedAggregation.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredNetworkWordCountWindowed.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredKafkaWordCount.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredNetworkWordCount.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredSessionization.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/pythonconverters/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/pythonconverters/AvroConverters.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LocalLR.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkTC.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/BroadcastTest.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ExceptionHandlingTest.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LocalKMeans.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/AggregateMessagesExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/Analytics.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/SSSPExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/SynthBenchmark.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/ConnectedComponentsExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/ComprehensiveExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/PageRankExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/TriangleCountingExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/LiveJournalPageRank.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/HdfsTest.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SimpleSkewedGroupByTest.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkPageRank.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/StatefulNetworkWordCount.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/HdfsWordCount.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/DirectKafkaWordCount.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/QueueStream.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/FlumePollingEventCount.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/SqlNetworkWordCount.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/FlumeEventCount.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/RecoverableNetworkWordCount.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/clickstream/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/clickstream/PageViewStream.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/clickstream/PageViewGenerator.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/StreamingExamples.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/NetworkWordCount.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/CustomReceiver.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/KafkaWordCount.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/RawNetworkGrep.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkPi.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkALS.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LocalFileLR.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/DriverSubmissionTest.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LogQuery.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SVMWithSGDExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/LatentDirichletAllocationExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingLinearRegressionExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/MultiLabelMetricsExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RankingMetricsExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/AbstractParams.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StandardScalerExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StratifiedSamplingExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/BisectingKMeansExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingKMeansExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/ElementwiseProductExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/GradientBoostingClassificationExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RandomForestClassificationExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingTestExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/KernelDensityEstimationExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/TFIDFExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/GradientBoostingRegressionExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/TallSkinnyPCA.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PMMLModelExportExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/LogisticRegressionWithLBFGSExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PCAExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/AssociationRulesExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/DecisionTreeRunner.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/TallSkinnySVD.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/IsotonicRegressionExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/BinaryClassificationMetricsExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/HypothesisTestingKolmogorovSmirnovTestExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/LBFGSExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PrefixSpanExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/ChiSqSelectorExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/Correlations.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RecommendationExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SVDExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/MovieLensALS.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/CosineSimilarity.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/MulticlassMetricsExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PCAOnSourceVectorExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/Word2VecExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingLogisticRegression.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/NaiveBayesExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RegressionMetricsExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SampledRDDs.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/LinearRegressionWithSGDExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/DecisionTreeClassificationExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RandomRDDGeneration.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/KMeansExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SparseNaiveBayes.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/BinaryClassification.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PowerIterationClusteringExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/DenseKMeans.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/MultivariateSummarizer.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PCAOnRowMatrixExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/NormalizerExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/HypothesisTestingExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RandomForestRegressionExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/FPGrowthExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/GaussianMixtureExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SummaryStatisticsExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/CorrelationsExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/GradientBoostedTreesRunner.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/DecisionTreeRegressionExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/LinearRegression.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SimpleFPGrowth.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/LDAExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkLR.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LocalPi.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkHdfsLR.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SkewedGroupByTest.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LocalALS.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/resources/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/resources/people.json\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/resources/employees.json\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/resources/people.txt\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/resources/full_user.avsc\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/resources/kv1.txt\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/resources/users.parquet\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/resources/user.avsc\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/resources/users.avro\n",
            "spark-2.2.0-bin-hadoop2.7/examples/jars/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/jars/scopt_2.11-3.3.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/examples/jars/spark-examples_2.11-2.2.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/data/\n",
            "spark-2.2.0-bin-hadoop2.7/data/graphx/\n",
            "spark-2.2.0-bin-hadoop2.7/data/graphx/followers.txt\n",
            "spark-2.2.0-bin-hadoop2.7/data/graphx/users.txt\n",
            "spark-2.2.0-bin-hadoop2.7/data/streaming/\n",
            "spark-2.2.0-bin-hadoop2.7/data/streaming/AFINN-111.txt\n",
            "spark-2.2.0-bin-hadoop2.7/data/mllib/\n",
            "spark-2.2.0-bin-hadoop2.7/data/mllib/pagerank_data.txt\n",
            "spark-2.2.0-bin-hadoop2.7/data/mllib/kmeans_data.txt\n",
            "spark-2.2.0-bin-hadoop2.7/data/mllib/streaming_kmeans_data_test.txt\n",
            "spark-2.2.0-bin-hadoop2.7/data/mllib/sample_lda_libsvm_data.txt\n",
            "spark-2.2.0-bin-hadoop2.7/data/mllib/sample_kmeans_data.txt\n",
            "spark-2.2.0-bin-hadoop2.7/data/mllib/pic_data.txt\n",
            "spark-2.2.0-bin-hadoop2.7/data/mllib/sample_isotonic_regression_libsvm_data.txt\n",
            "spark-2.2.0-bin-hadoop2.7/data/mllib/als/\n",
            "spark-2.2.0-bin-hadoop2.7/data/mllib/als/test.data\n",
            "spark-2.2.0-bin-hadoop2.7/data/mllib/als/sample_movielens_ratings.txt\n",
            "spark-2.2.0-bin-hadoop2.7/data/mllib/sample_fpgrowth.txt\n",
            "spark-2.2.0-bin-hadoop2.7/data/mllib/sample_libsvm_data.txt\n",
            "spark-2.2.0-bin-hadoop2.7/data/mllib/ridge-data/\n",
            "spark-2.2.0-bin-hadoop2.7/data/mllib/ridge-data/lpsa.data\n",
            "spark-2.2.0-bin-hadoop2.7/data/mllib/sample_multiclass_classification_data.txt\n",
            "spark-2.2.0-bin-hadoop2.7/data/mllib/sample_linear_regression_data.txt\n",
            "spark-2.2.0-bin-hadoop2.7/data/mllib/sample_binary_classification_data.txt\n",
            "spark-2.2.0-bin-hadoop2.7/data/mllib/sample_lda_data.txt\n",
            "spark-2.2.0-bin-hadoop2.7/data/mllib/sample_movielens_data.txt\n",
            "spark-2.2.0-bin-hadoop2.7/data/mllib/sample_svm_data.txt\n",
            "spark-2.2.0-bin-hadoop2.7/data/mllib/gmm_data.txt\n",
            "spark-2.2.0-bin-hadoop2.7/R/\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/sparkr.zip\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/groupBy.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/covar_pop.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sampleBy.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sql.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/year.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/tan.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/GBTRegressionModel-class.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/last_day.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sign.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/hint.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/randn.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/orderBy.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/otherwise.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/AFTSurvivalRegressionModel-class.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/hashCode.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/bin.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/dim.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.svmLinear.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/minute.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/createExternalTable-deprecated.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/distinct.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sparkR.conf.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/print.jobj.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/md5.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/cbrt.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/write.ml.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/dapplyCollect.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/acos.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/tables.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/NaiveBayesModel-class.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sum.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/structType.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/hex.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/isLocal.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/write.jdbc.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/from_utc_timestamp.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/tableNames.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/createDataFrame.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/isStreaming.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/toJSON.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.getSparkFiles.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/except.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/LDAModel-class.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/months_between.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark_partition_id.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/write.parquet.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sumDistinct.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/awaitTermination.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/BisectingKMeansModel-class.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sha2.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/abs.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/date_format.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/withColumn.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/dayofyear.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sort_array.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/storageLevel.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/setCurrentDatabase.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/ceil.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/floor.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/stddev_pop.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sd.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/print.structType.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.survreg.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/predict.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/count.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/unhex.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/mean.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/instr.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/from_unixtime.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/saveAsTable.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/ltrim.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sparkRHive.init-deprecated.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/read.parquet.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/match.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/is.nan.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/read.ml.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/lag.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/write.json.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/unpersist.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/corr.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sparkR.callJStatic.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/rint.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/LinearSVCModel-class.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.gbt.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/RandomForestClassificationModel-class.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/persist.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/var_pop.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/selectExpr.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/crc32.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/expr.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sparkR.callJMethod.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/with.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/generateAliasesForIntersectedCols.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/GeneralizedLinearRegressionModel-class.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/IsotonicRegressionModel-class.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/setLogLevel.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/shiftRightUnsigned.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/base64.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/array_contains.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/expm1.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/write.orc.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sparkR.version.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/insertInto.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/SparkDataFrame.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/merge.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/dayofmonth.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/listDatabases.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/summarize.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/format_number.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/from_json.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/dropDuplicates.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/cache.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/write.text.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/approxCountDistinct.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sparkRSQL.init-deprecated.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/LogisticRegressionModel-class.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/stddev_samp.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/pivot.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/showDF.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/between.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/struct.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/subset.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/posexplode.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/lit.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/shiftLeft.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/glm.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/hypot.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/recoverPartitions.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sparkR.session.stop.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/translate.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/drop.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/GBTClassificationModel-class.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/shiftRight.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/regexp_replace.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/randomSplit.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/length.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/rowsBetween.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/read.jdbc.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/schema.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/toRadians.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/filter.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/bround.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/createOrReplaceTempView.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/cancelJobGroup.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/second.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/upper.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/head.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/limit.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/concat_ws.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/when.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/FPGrowthModel-class.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/install.spark.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sparkR.newJObject.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/dropTempView.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/unbase64.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/soundex.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/structField.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.addFile.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.bisectingKmeans.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/cacheTable.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/cosh.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.mlp.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/ntile.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/atan2.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.kstest.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/dtypes.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/reverse.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sinh.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.lda.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/createTable.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/negate.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/asin.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/hash.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/rank.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/toDegrees.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/columns.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/columnfunctions.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/substring_index.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/to_timestamp.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.naiveBayes.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/atan.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.isoreg.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/factorial.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/countDistinct.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/quarter.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/setCheckpointDir.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/least.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/read.text.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/windowOrderBy.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/dapply.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/coalesce.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/refreshByPath.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/cume_dist.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/dense_rank.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/freqItems.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/getNumPartitions.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/KMeansModel-class.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sparkR.session.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/arrange.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/write.stream.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/encode.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.glm.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/isActive.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/crossJoin.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/rpad.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/uncacheTable.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/size.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/conv.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/log10.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/collect.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/read.stream.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/format_string.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/windowPartitionBy.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/union.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/stopQuery.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/dropTempTable-deprecated.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/endsWith.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/startsWith.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/nanvl.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/mutate.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/explain.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/R.css\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/cov.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/var_samp.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/log2.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/registerTempTable-deprecated.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/lastProgress.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/attach.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/min.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/ncol.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/month.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/window.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/partitionBy.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/percent_rank.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/listFunctions.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/to_utc_timestamp.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/crosstab.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/take.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/exp.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/to_json.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/column.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/ifelse.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/GaussianMixtureModel-class.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.logit.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/show.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/setJobGroup.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/KSTest-class.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/clearJobGroup.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/last.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/printSchema.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/rename.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/rbind.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/over.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/MultilayerPerceptronClassificationModel-class.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/coltypes.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/datediff.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/lead.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/summary.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/WindowSpec.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/unix_timestamp.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/tanh.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/listColumns.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/to_date.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/00Index.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sparkR.uiWebUrl.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/max.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/var.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/print.structField.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.als.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/alias.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sha1.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/status.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/read.orc.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/currentDatabase.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/write.df.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/round.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sparkR.init-deprecated.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/refreshTable.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/nrow.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/pmod.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/intersect.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/rtrim.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/substr.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/log.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/levenshtein.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/monotonically_increasing_id.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/checkpoint.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/decode.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.lapply.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/read.json.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/RandomForestRegressionModel-class.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/trim.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/select.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/regexp_extract.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/as.data.frame.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/rangeBetween.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/queryName.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sample.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/lower.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/repartition.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/concat.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/cast.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/date_add.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.fpGrowth.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/hour.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/initcap.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/explode.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/add_months.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/bitwiseNOT.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/approxQuantile.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/kurtosis.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/greatest.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/first.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/read.df.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/rand.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/next_day.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/clearCache.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/nafunctions.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/row_number.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/lpad.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/skewness.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/gapplyCollect.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/locate.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/avg.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sqrt.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/cos.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/log1p.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/str.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/StreamingQuery.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/ALSModel-class.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/ascii.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/listTables.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sin.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/histogram.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/weekofyear.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/GroupedData.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/fitted.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/date_sub.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/tableToDF.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.getSparkFilesRootDirectory.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/join.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/gapply.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.randomForest.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.kmeans.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.gaussianMixture.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/INDEX\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/R/\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/R/SparkR.rdx\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/R/SparkR\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/R/SparkR.rdb\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/help/\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/help/aliases.rds\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/help/paths.rds\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/help/SparkR.rdx\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/help/AnIndex\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/help/SparkR.rdb\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/DESCRIPTION\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/Meta/\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/Meta/hsearch.rds\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/Meta/nsInfo.rds\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/Meta/package.rds\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/Meta/links.rds\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/Meta/Rd.rds\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/profile/\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/profile/general.R\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/profile/shell.R\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/worker/\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/worker/daemon.R\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/worker/worker.R\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/NAMESPACE\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/tests/\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/tests/testthat/\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/tests/testthat/test_basic.R\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-slf4j.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-scalacheck.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-py4j.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-paranamer.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-netlib.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-Mockito.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-minlog.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-graphlib-dot.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-d3.min.js.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-f2j.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-junit-interface.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-boto.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-spire.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-pyrolite.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-xmlenc.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-scala.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-SnapTree.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-AnchorJS.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-jline.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-heapq.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-cloudpickle.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-reflectasm.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-jpmml-model.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-modernizr.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-javolution.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-jbcrypt.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-sbt-launch-lib.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-postgresql.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-kryo.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-DPark.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-scopt.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-sorttable.js.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-dagre-d3.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-jquery.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-protobuf.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-antlr.txt\n",
            "spark-2.2.0-bin-hadoop2.7/conf/\n",
            "spark-2.2.0-bin-hadoop2.7/conf/fairscheduler.xml.template\n",
            "spark-2.2.0-bin-hadoop2.7/conf/metrics.properties.template\n",
            "spark-2.2.0-bin-hadoop2.7/conf/spark-env.sh.template\n",
            "spark-2.2.0-bin-hadoop2.7/conf/log4j.properties.template\n",
            "spark-2.2.0-bin-hadoop2.7/conf/docker.properties.template\n",
            "spark-2.2.0-bin-hadoop2.7/conf/slaves.template\n",
            "spark-2.2.0-bin-hadoop2.7/conf/spark-defaults.conf.template\n",
            "spark-2.2.0-bin-hadoop2.7/LICENSE\n",
            "spark-2.2.0-bin-hadoop2.7/bin/\n",
            "spark-2.2.0-bin-hadoop2.7/bin/spark-shell\n",
            "spark-2.2.0-bin-hadoop2.7/bin/spark-submit.cmd\n",
            "spark-2.2.0-bin-hadoop2.7/bin/spark-shell2.cmd\n",
            "spark-2.2.0-bin-hadoop2.7/bin/pyspark\n",
            "spark-2.2.0-bin-hadoop2.7/bin/sparkR.cmd\n",
            "spark-2.2.0-bin-hadoop2.7/bin/spark-class2.cmd\n",
            "spark-2.2.0-bin-hadoop2.7/bin/run-example.cmd\n",
            "spark-2.2.0-bin-hadoop2.7/bin/spark-submit2.cmd\n",
            "spark-2.2.0-bin-hadoop2.7/bin/spark-class\n",
            "spark-2.2.0-bin-hadoop2.7/bin/spark-submit\n",
            "spark-2.2.0-bin-hadoop2.7/bin/spark-sql\n",
            "spark-2.2.0-bin-hadoop2.7/bin/find-spark-home\n",
            "spark-2.2.0-bin-hadoop2.7/bin/run-example\n",
            "spark-2.2.0-bin-hadoop2.7/bin/beeline\n",
            "spark-2.2.0-bin-hadoop2.7/bin/pyspark2.cmd\n",
            "spark-2.2.0-bin-hadoop2.7/bin/spark-shell.cmd\n",
            "spark-2.2.0-bin-hadoop2.7/bin/spark-class.cmd\n",
            "spark-2.2.0-bin-hadoop2.7/bin/pyspark.cmd\n",
            "spark-2.2.0-bin-hadoop2.7/bin/sparkR\n",
            "spark-2.2.0-bin-hadoop2.7/bin/beeline.cmd\n",
            "spark-2.2.0-bin-hadoop2.7/bin/sparkR2.cmd\n",
            "spark-2.2.0-bin-hadoop2.7/bin/load-spark-env.sh\n",
            "spark-2.2.0-bin-hadoop2.7/bin/load-spark-env.cmd\n",
            "spark-2.2.0-bin-hadoop2.7/yarn/\n",
            "spark-2.2.0-bin-hadoop2.7/yarn/spark-2.2.0-yarn-shuffle.jar\n",
            "spark-2.2.0-bin-hadoop2.7/README.md\n",
            "環境初始化完畢\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ky97pnWo7EBr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os, sys\n",
        "os.environ['SPARK_HOME'] = \"/usr/local/spark\"\n",
        "os.environ['PYSPARK_PYTHON'] = \"/usr/local/bin/python\"\n",
        "sys.path.append(\"/usr/local/spark/python/\")\n",
        "sys.path.append(\"/usr/local/spark/python/lib/pyspark.zip\")\n",
        "sys.path.append(\"/usr/local/spark/python/lib/py4j-0.10.4-src.zip\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1kaE7kK7PRF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark import SparkContext\n",
        "from pyspark import SparkConf\n",
        "sc =SparkContext() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltQpoNZY6bv6",
        "colab_type": "text"
      },
      "source": [
        "# 利用Spark 分析台灣2015 PM2.5資料集"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbWZ0dIo6bv7",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"load_data\"></a>\n",
        "## 上傳台灣2015一整年空氣監測資料 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1qgJDsb6bv8",
        "colab_type": "text"
      },
      "source": [
        "### 步驟1: 將2015空氣監控資料，上傳至colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVXc7N-P6bv8",
        "colab_type": "code",
        "outputId": "7bac9b6a-fdc2-44eb-8829-df6f296f97b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "!wget -O pm25.csv \"https://www.dropbox.com/s/zkn3ba7pitv83el/pm2.5Taiwan.csv?dl=0\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-10-27 08:12:32--  https://www.dropbox.com/s/zkn3ba7pitv83el/pm2.5Taiwan.csv?dl=0\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.1.1, 2620:100:601b:1::a27d:801\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.1.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/zkn3ba7pitv83el/pm2.5Taiwan.csv [following]\n",
            "--2019-10-27 08:12:32--  https://www.dropbox.com/s/raw/zkn3ba7pitv83el/pm2.5Taiwan.csv\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ucb5fcc380808e02ad4970f9fd8f.dl.dropboxusercontent.com/cd/0/inline/ArMaM5GCj7A46mSM4i3Gm7G1VfEMzSgqRuA2zxajWYRFvYRgpVILW81qOOmk4qRYA08rn49D7GorVbk83VM6C-nuLm8kE5SSCVO4hr1E0-hK-A/file# [following]\n",
            "--2019-10-27 08:12:33--  https://ucb5fcc380808e02ad4970f9fd8f.dl.dropboxusercontent.com/cd/0/inline/ArMaM5GCj7A46mSM4i3Gm7G1VfEMzSgqRuA2zxajWYRFvYRgpVILW81qOOmk4qRYA08rn49D7GorVbk83VM6C-nuLm8kE5SSCVO4hr1E0-hK-A/file\n",
            "Resolving ucb5fcc380808e02ad4970f9fd8f.dl.dropboxusercontent.com (ucb5fcc380808e02ad4970f9fd8f.dl.dropboxusercontent.com)... 162.125.8.6, 2620:100:6016:6::a27d:106\n",
            "Connecting to ucb5fcc380808e02ad4970f9fd8f.dl.dropboxusercontent.com (ucb5fcc380808e02ad4970f9fd8f.dl.dropboxusercontent.com)|162.125.8.6|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 50453822 (48M) [text/plain]\n",
            "Saving to: ‘pm25.csv’\n",
            "\n",
            "pm25.csv            100%[===================>]  48.12M  53.7MB/s    in 0.9s    \n",
            "\n",
            "2019-10-27 08:12:34 (53.7 MB/s) - ‘pm25.csv’ saved [50453822/50453822]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYAxfnu1SkXO",
        "colab_type": "code",
        "outputId": "13380c93-1cb0-470a-da52-c672bf9972cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "init_env.sh  pm25.csv  sample_data  spark-2.2.0-bin-hadoop2.7.tgz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8u7Bxqi6bwF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weather = sc.textFile(\"./pm25.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMVBdYNO6bwH",
        "colab_type": "text"
      },
      "source": [
        "### 步驟2: 試試看是否成功上傳  (使用count( ), first( ), collect( ), take( ) )"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeiPKyGe6bwH",
        "colab_type": "code",
        "outputId": "948b8bdd-f185-40cc-ad5d-3c85f90d29ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "weather.count()\n",
        "weather.first()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'日期,測站,測項,00,01,02,03,04,05,06,07,08,09,10,11,12,13,14,15,16,17,18,19,20,21,22,23'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6xuyI9R6bwL",
        "colab_type": "code",
        "outputId": "c03a6733-1527-4693-c0b3-0c9719f36a74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "weather.take(5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['日期,測站,測項,00,01,02,03,04,05,06,07,08,09,10,11,12,13,14,15,16,17,18,19,20,21,22,23',\n",
              " '2015/01/01,龍潭,AMB_TEMP,14,14,14,13,13,13,12,12,13,14,14,14,14,14,13,13,12,11,11,11,11,11,11,11',\n",
              " '2015/01/01,龍潭,CO,0.69,0.72,0.69,0.64,0.54,0.47,0.45,0.48,0.51,0.54,0.54,0.5,0.47,0.38,0.36,0.35,0.34,0.37,0.34,0.29,0.26,0.22,0.19,0.18',\n",
              " '2015/01/01,龍潭,NO,0.3,0.1,0.6,2,2,1.9,2.2,3.1,3.7,4.3,4.3,4.5,3.3,4.1,3.1,3.6,3.6,2.8,2.8,2.5,2.2,1.4,2.1,2',\n",
              " '2015/01/01,龍潭,NO2,11,9.6,8.7,9.1,9.6,9.9,11,13,11,12,12,11,11,9.9,9.9,10,11,13,11,10,8.2,7.3,6.5,5.5']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lZMZ1m06bwO",
        "colab_type": "text"
      },
      "source": [
        "# 練習1: 讓我們求取2015年，大里每小時的平均pm25數值。\n",
        "## 注意事項：\n",
        "1. 資料分割：原始資料每一行為一個觀測值，我們必須將資料進行分割，才能逐一計算與進行操作。\n",
        "2. 資料清洗：在氣象局的原始資料裡，有些數值由於當初偵測時有異常，所以會加註特別符號如\\*\\#等特殊符號，這些數值我們必須先經過前處理，我們才能進行算術運算。\n",
        "3. 資料選擇：將大里資料挑選出來\n",
        "4. 產生key-value，也就是(小時,pm25值)\n",
        "5. 利用flatMap(), reduceByKey(), groupByKey()，將不同日期但相同時間的pm25值收集起來。\n",
        "6. 計算平均值, 標準差, 最大最小值。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7aedWEz6bwO",
        "colab_type": "text"
      },
      "source": [
        "### 步驟一：資料分割 (使用map () 與 split( ))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DDuC6At6bwP",
        "colab_type": "code",
        "outputId": "3950a67d-8b04-4652-9ec6-ff48ed446943",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "weatherParse = weather.map(lambda line : line.split(\",\"))\n",
        "print(weatherParse.first())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['日期', '測站', '測項', '00', '01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CB_BMIXf6bwR",
        "colab_type": "text"
      },
      "source": [
        "### 步驟二：將大里站資料從全部資料集中挑選出來 (filter)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-np2VJz6bwR",
        "colab_type": "text"
      },
      "source": [
        "須留意unicode與string的差別,  u'大里'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwzyA92o6bwS",
        "colab_type": "code",
        "outputId": "1c645059-2617-4a59-b194-34d8b4aec222",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "wea_dali = weatherParse.filter(lambda x: x[1] == '大里' and x[2]== \"PM2.5\")\n",
        "print(wea_dali.take(50))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['2015/01/01', '大里', 'PM2.5', '53', '55', '58', '53', '43', '36', '35', '42', '55', '64', '65', '59', '52', '44', '47', '41', '43', '40', '42', '35', '28', '20', '18', '16'], ['2015/01/02', '大里', 'PM2.5', '21', '22', '26', '23', '20', '18', '15', '21', '21', '25', '29', '32', '34', '29', '32', '39', '51', '51', '47', '43', '43', '48', '47', '53'], ['2015/01/03', '大里', 'PM2.5', '48', '48', '43', '38', '37', '36', '37', '34', '37', '46', '64', '77', '83', '75', '68', '69', '64', '65', '59', '66', '71', '66', '57', '48'], ['2015/01/04', '大里', 'PM2.5', '60', '56', '53', '43', '53', '53', '52', '44', '44', '50', '49', '51', '45', '42', '40', '38', '36', '43', '51', '63', '68', '72', '66', '58'], ['2015/01/05', '大里', 'PM2.5', '48', '42', '42', '34', '34', '28', '34', '35', '45', '47', '54', '46', '35', '19', '16', '21', '24', '28', '37', '52', '60', '62', '64', '61'], ['2015/01/06', '大里', 'PM2.5', '59', '40', '34', '25', '27', '29', '26', '33', '42', '47', '38', '24', '14', '8', '17', '30', '51', '62', '68', '83', '83', '96', '103', '110'], ['2015/01/07', '大里', 'PM2.5', '117', '110', '97', '68', '47', '39', '34', '27', '22', '15', '14', '', '23', '18', '16', '12', '10', '6', '5', '9', '15', '21', '23', '15'], ['2015/01/08', '大里', 'PM2.5', '7', '9', '13', '18', '11', '12', '17', '29', '34', '39', '41', '46', '46', '44', '43', '39', '41', '46', '47', '48', '47', '47', '43', '33'], ['2015/01/09', '大里', 'PM2.5', '35', '34', '37', '30', '25', '25', '22', '21', '18', '20', '14', '12', '21', '31', '44', '46', '52', '44', '39', '37', '43', '43', '42', '39'], ['2015/01/10', '大里', 'PM2.5', '38', '33', '31', '24', '20', '19', '22', '31', '31', '45', '48', '49', '38', '39', '43', '46', '43', '36', '33', '29', '37', '34', '39', '33'], ['2015/01/11', '大里', 'PM2.5', '37', '41', '43', '43', '27', '22', '26', '34', '39', '37', '51', '53', '61', '56', '48', '43', '37', '43', '43', '48', '54', '51', '46', '35'], ['2015/01/12', '大里', 'PM2.5', '36', '40', '33', '32', '33', '40', '37', '34', '39', '53', '60', '65', '57', '50', '52', '51', '43', '24', '20', '28', '35', '40', '30', '36'], ['2015/01/13', '大里', 'PM2.5', '36', '36', '32', '33', '38', '45', '38', '45', '45', '76', '84', '96', '92', '87', '64', '33', '21', '22', '20', '15', '7', '12', '9', '11'], ['2015/01/14', '大里', 'PM2.5', '10', '7', '3', '0', '3', '7', '5', '1', '0', '0', '0', '0', '0', '0', '4', '12', '13', '10', '12', '14', '21', '19', '15', '8'], ['2015/01/15', '大里', 'PM2.5', '2', '3', '7', '3', '7', '5', '10', '7', '13', '16', '14', '8', '5', '13', '20', '30', '30', '33', '28', '29', '33', '26', '23', '12'], ['2015/01/16', '大里', 'PM2.5', '16', '15', '17', '16', '16', '13', '5', '10', '14', '30', '30', '25', '-4#', '22', '23', '30', '33', '40', '43', '45', '37', '34', '38', '43'], ['2015/01/17', '大里', 'PM2.5', '42', '33', '25', '18', '13', '9', '12', '20', '28', '33', '33', '43', '52', '55', '57', '60', '66', '77', '76', '76', '77', '74', '82', '75'], ['2015/01/18', '大里', 'PM2.5', '77', '66', '61', '62', '70', '71', '69', '71', '75', '82', '90', '94', '88', '75', '57', '47', '33', '31', '25', '21', '16', '16', '18', '16'], ['2015/01/19', '大里', 'PM2.5', '14', '14', '19', '18', '16', '7', '6', '12', '18', '30', '26', '28', '31', '32', '35', '37', '41', '43', '42', '50', '53', '54', '57', '62'], ['2015/01/20', '大里', 'PM2.5', '62', '57', '52', '53', '61', '62', '62', '65', '73', '82', '83', '88', '83', '81', '73', '71', '69', '71', '66', '55', '49', '42', '49', '43'], ['2015/01/21', '大里', 'PM2.5', '46', '37', '30', '31', '31', '38', '42', '46', '46', '36', '33', '33', '37', '34', '33', '33', '34', '39', '48', '50', '47', '45', '43', '39'], ['2015/01/22', '大里', 'PM2.5', '36', '46', '51', '43', '33', '27', '36', '36', '43', '29', '29', '26', '32', '31', '42', '', '73', '73', '80', '82', '86', '70', '74', '66'], ['2015/01/23', '大里', 'PM2.5', '61', '47', '46', '55', '49', '39', '29', '31', '29', '19', '12', '14', '30', '49', '57', '55', '57', '56', '64', '72', '73', '75', '68', '68'], ['2015/01/24', '大里', 'PM2.5', '63', '61', '59', '58', '56', '52', '39', '33', '34', '40', '30', '33', '49', '66', '76', '75', '73', '60', '58', '67', '83', '88', '87', '80'], ['2015/01/25', '大里', 'PM2.5', '71', '68', '65', '63', '63', '62', '57', '55', '65', '70', '79', '75', '75', '64', '59', '68', '78', '86', '80', '74', '66', '60', '49', '44'], ['2015/01/26', '大里', 'PM2.5', '50', '53', '48', '43', '38', '40', '38', '47', '49', '59', '55', '53', '39', '29', '28', '38', '51', '55', '53', '55', '54', '61', '55', '61'], ['2015/01/27', '大里', 'PM2.5', '56', '52', '41', '46', '49', '52', '43', '43', '48', '48', '40', '30', '30', '33', '36', '37', '35', '36', '43', '46', '42', '30', '22', '17'], ['2015/01/28', '大里', 'PM2.5', '17', '9', '12', '6', '7', '2', '0', '0', '0', '0', '2', '7', '8', '13', '11', '15', '12', '20', '23', '21', '13', '7', '11', '12'], ['2015/01/29', '大里', 'PM2.5', '14', '14', '12', '7', '1', '0', '4', '6', '12', '16', '15', '', '52x', '38x', '30', '29', '27', '29', '24', '24', '21', '19', '19', '23'], ['2015/01/30', '大里', 'PM2.5', '17', '15', '13', '15', '14', '14', '9', '8', '10', '12', '12', '8', '7', '12', '16', '25', '24', '24', '14', '10', '5', '3', '7', '6'], ['2015/01/31', '大里', 'PM2.5', '6', '0', '1', '9', '9', '10', '7', '12', '12', '10', '17', '23', '26', '27', '28', '33', '31', '35', '30', '33', '23', '16', '8', '8'], ['2015/02/01', '大里', 'PM2.5', '17', '11', '8', '2', '3', '1', '0', '2', '5', '10', '8', '10', '23', '26', '23', '13', '11', '16', '18', '23', '23', '15', '16', '12'], ['2015/02/02', '大里', 'PM2.5', '15', '12', '18', '25', '25', '25', '25', '32', '37', '39', '45', '47', '49', '45', '36', '30', '21', '24', '23', '27', '24', '19', '10', '15'], ['2015/02/03', '大里', 'PM2.5', '23', '27', '25', '22', '17', '17', '16', '23', '26', '31', '28', '20', '17', '22', '27', '25', '23', '13', '5', '3', '2', '2', '2', '10'], ['2015/02/04', '大里', 'PM2.5', '10', '12', '9', '12', '7', '4', '4', '2', '13', '20', '26', '16', '10', '19', '23', '26', '21', '26', '33', '28', '29', '27', '31', '26'], ['2015/02/05', '大里', 'PM2.5', '21', '28', '25', '33', '33', '37', '32', '19', '19', '24', '39', '53', '65', '', '', '93', '93', '90', '86', '80', '76', '75', '80', '71'], ['2015/02/06', '大里', 'PM2.5', '63', '53', '52', '51', '51', '47', '43', '30', '27', '28', '', '41', '56', '60', '59', '53', '53', '51', '44', '41', '39', '41', '35', '31'], ['2015/02/07', '大里', 'PM2.5', '32', '32', '38', '43', '49', '48', '47', '54', '56', '60', '62', '66', '70', '73', '60', '46', '23', '25', '23', '40', '36', '32', '25', '29'], ['2015/02/08', '大里', 'PM2.5', '39', '38', '34', '28', '14', '15', '13', '23', '18', '24', '26', '32', '31', '33', '36', '39', '38', '49', '53', '60', '53', '43', '34', '28'], ['2015/02/09', '大里', 'PM2.5', '23', '15', '6', '3', '6', '9', '9', '7', '8', '7', '12', '12', '12', '7', '9', '9', '12', '11', '15', '12', '16', '11', '15', '8'], ['2015/02/10', '大里', 'PM2.5', '9', '3', '7', '3', '4', '0', '0', '0', '3', '12', '', '11', '20', '22', '28', '26', '30', '32', '24', '14', '17', '19', '29', '24'], ['2015/02/11', '大里', 'PM2.5', '24', '23', '24', '27', '23', '11', '16', '14', '32', '33', '55', '58', '55', '37', '24', '23', '23', '30', '28', '28', '27', '32', '44', '47'], ['2015/02/12', '大里', 'PM2.5', '54', '48', '47', '43', '42', '43', '43', '55', '54', '53', '51', '', '48', '45', '39', '45', '48', '47', '47', '44', '49', '56', '64', '61'], ['2015/02/13', '大里', 'PM2.5', '51', '49', '44', '46', '39', '39', '37', '38', '53', '54', '53', '39', '39', '44', '50', '50', '53', '63', '74', '75', '68', '70', '62', '62'], ['2015/02/14', '大里', 'PM2.5', '54', '57', '53', '52', '58', '57', '53', '53', '74', '82', '81', '72', '68', '73', '58', '53', '35', '40', '48', '50', '59', '57', '67', '68'], ['2015/02/15', '大里', 'PM2.5', '66', '83', '88', '94', '81', '64', '58', '53', '51', '57', '48', '53', '43', '70', '94', '104', '86', '67', '58', '70', '79', '79', '74', '65'], ['2015/02/16', '大里', 'PM2.5', '70', '67', '63', '55', '39', '28', '28', '46', '67x', '64x', '', '41', '33', '21', '24', '18', '18', '16', '23', '27', '27', '38', '41', '44'], ['2015/02/17', '大里', 'PM2.5', '43', '34', '37', '32', '36', '31', '23', '28', '33', '49', '60', '70', '77', '62', '54', '41', '46', '43', '41', '47', '52', '59', '58', '63'], ['2015/02/18', '大里', 'PM2.5', '67', '71', '70', '67', '59', '55', '51', '53', '53', '55', '49', '41', '38', '44', '51', '53', '53', '57', '49', '50', '48', '47', '47', '42'], ['2015/02/19', '大里', 'PM2.5', '58', '60', '62', '47', '49', '43', '37', '39', '47', '58', '54', '52', '51', '46', '43', '35', '29', '31', '27', '33', '33', '43', '40', '50']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImDZp8136bwV",
        "colab_type": "text"
      },
      "source": [
        "### 步驟三：資料清洗 (使用 map(), str.strip())\n",
        "#### 在氣象局的原始資料裡，有些數值由於當初偵測時有異常，所以會加註特別符號如\\*\\#等特殊符號，或者沒有取到數值為一空值，這些數值我們必須先經過前處理，我們才能進行算術運算。\n",
        "1. ```2015/01/29 大里 PM2.5 14 14 12 7 1 0 4 6 12 16 15  52x 38x 30 29 27 29 24 24 21 19 19 23```\n",
        "2. ```2015/01/16 大里 PM2.5 16 15 17 16 16 13 5 10 14 30 30 25 -4# 22 23 30 33 40 43 45 37 34 38 43``` "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uqf9UPnz6bwV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def strip_symbol(x):\n",
        "    for i in range(len(x)):\n",
        "        x[i] = x[i].strip(\"-*#x\") # remove non-digits\n",
        "        if x[i]==\"\": x[i]=\"0\"\n",
        "        #if float(x[i]) > 150 : x[i] = \"0\"\n",
        "    return x\n",
        "def over(x):\n",
        "  for i in range(len(x)-3):\n",
        "    if float(x[i+3]) > 500 : x[i+3] = \"0\"\n",
        "  return x\n",
        "wea_dali = wea_dali.map(strip_symbol)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWnybfVB6bwY",
        "colab_type": "text"
      },
      "source": [
        "### 步驟四：key value pair的產生 (重要的操作概念)\n",
        "*將每小時資料轉成(小時,pm數值)，以求取每小時的平均值。\n",
        "\n",
        "例如：\n",
        "    2015/01/01 大里 PM2.5 53 55 58 53 43 36 35 42 55 64 65 59 52 44 47 41 43 40 42 35 28 20 18 16\n",
        "    --> [(0, 53) (1, 55) (2, 58) (3, 53) (4, 43) ... (21, 20) (22, 18) (23, 16)]\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGjDPGJl6bwZ",
        "colab_type": "code",
        "outputId": "3f855cee-6f58-47e7-d596-08e4055cb843",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(wea_dali.first())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['2015/01/01', '大里', 'PM2.5', '53', '55', '58', '53', '43', '36', '35', '42', '55', '64', '65', '59', '52', '44', '47', '41', '43', '40', '42', '35', '28', '20', '18', '16']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPJlnWbz6bwa",
        "colab_type": "code",
        "outputId": "11b63912-656b-405b-d3f0-571b5bce6039",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "def hourKeyGen(x):\n",
        "    hourkeypair = []\n",
        "    x=x[3:]\n",
        "    for i, value in enumerate(x):\n",
        "      print(i, value)\n",
        "      hourkeypair.append((i, float(value)))\n",
        "    return hourkeypair\n",
        "\n",
        "wea_dali_byHourkey = wea_dali.map(hourKeyGen)\n",
        "\n",
        "wea_dali_byHourkey.first()\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0, 53.0),\n",
              " (1, 55.0),\n",
              " (2, 58.0),\n",
              " (3, 53.0),\n",
              " (4, 43.0),\n",
              " (5, 36.0),\n",
              " (6, 35.0),\n",
              " (7, 42.0),\n",
              " (8, 55.0),\n",
              " (9, 64.0),\n",
              " (10, 65.0),\n",
              " (11, 59.0),\n",
              " (12, 52.0),\n",
              " (13, 44.0),\n",
              " (14, 47.0),\n",
              " (15, 41.0),\n",
              " (16, 43.0),\n",
              " (17, 40.0),\n",
              " (18, 42.0),\n",
              " (19, 35.0),\n",
              " (20, 28.0),\n",
              " (21, 20.0),\n",
              " (22, 18.0),\n",
              " (23, 16.0)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNfl3Vio6bwd",
        "colab_type": "text"
      },
      "source": [
        "### 步驟五： 利用flatMap(), reduceByKey(), groupByKey()，將不同日期但相同時間的pm25值收集起來。(使用flatMap)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQWgeD-w6bwf",
        "colab_type": "code",
        "outputId": "aa147cc7-449f-4cf7-d415-9210bc3034c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "byHourkey = wea_dali.flatMap(hourKeyGen)\n",
        "byHourkey.reduceByKey(lambda x,y: x+y).take(10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0, 9838.0),\n",
              " (2, 9229.0),\n",
              " (4, 8346.0),\n",
              " (6, 8121.0),\n",
              " (8, 9580.0),\n",
              " (10, 10919.0),\n",
              " (12, 11505.0),\n",
              " (14, 10172.0),\n",
              " (16, 10396.0),\n",
              " (18, 10396.0)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtQpIRWJ6bwh",
        "colab_type": "text"
      },
      "source": [
        "### 步驟六： 計算大里區每個小時區間中，平均之pm25數值 (使用reduceByKey)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlXpUgc36bwi",
        "colab_type": "code",
        "outputId": "7f18ba3c-ac49-45a7-e1b2-d4002de2c45e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "avg_pm25_hour = byHourkey.reduceByKey(lambda x,y: x+y)\n",
        "avg_pm25_hour.map(lambda x:(x[0],x[1]/365)).collect()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0, 26.953424657534246),\n",
              " (2, 25.284931506849315),\n",
              " (4, 22.865753424657534),\n",
              " (6, 22.24931506849315),\n",
              " (8, 26.246575342465754),\n",
              " (10, 29.915068493150685),\n",
              " (12, 31.52054794520548),\n",
              " (14, 27.86849315068493),\n",
              " (16, 28.482191780821918),\n",
              " (18, 28.482191780821918),\n",
              " (20, 30.136986301369863),\n",
              " (22, 29.161643835616438),\n",
              " (1, 25.704109589041096),\n",
              " (3, 23.76164383561644),\n",
              " (5, 21.975342465753425),\n",
              " (7, 23.572602739726026),\n",
              " (9, 29.252054794520546),\n",
              " (11, 30.265753424657536),\n",
              " (13, 30.65205479452055),\n",
              " (15, 28.147945205479452),\n",
              " (17, 28.295890410958904),\n",
              " (19, 29.315068493150687),\n",
              " (21, 29.783561643835615),\n",
              " (23, 27.852054794520548)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mko2dCdz6bwk",
        "colab_type": "text"
      },
      "source": [
        "### 步驟七： 根據pm25平均濃度，進行排序。使用top( )"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qgZSxDs6bwl",
        "colab_type": "code",
        "outputId": "39088963-1807-4aa5-993e-a9fbf9242b49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "avg_pm25_hour = byHourkey.reduceByKey(lambda x,y: x+y)\n",
        "avg_pm25_hour.map(lambda x:(x[0],x[1]/365.0)).map(lambda x: (x[1],x[0])).top(24)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(31.52054794520548, 12),\n",
              " (30.65205479452055, 13),\n",
              " (30.265753424657536, 11),\n",
              " (30.136986301369863, 20),\n",
              " (29.915068493150685, 10),\n",
              " (29.783561643835615, 21),\n",
              " (29.315068493150687, 19),\n",
              " (29.252054794520546, 9),\n",
              " (29.161643835616438, 22),\n",
              " (28.482191780821918, 18),\n",
              " (28.482191780821918, 16),\n",
              " (28.295890410958904, 17),\n",
              " (28.147945205479452, 15),\n",
              " (27.86849315068493, 14),\n",
              " (27.852054794520548, 23),\n",
              " (26.953424657534246, 0),\n",
              " (26.246575342465754, 8),\n",
              " (25.704109589041096, 1),\n",
              " (25.284931506849315, 2),\n",
              " (23.76164383561644, 3),\n",
              " (23.572602739726026, 7),\n",
              " (22.865753424657534, 4),\n",
              " (22.24931506849315, 6),\n",
              " (21.975342465753425, 5)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sReXkMgl6bwo",
        "colab_type": "text"
      },
      "source": [
        "### 步驟八： 計算每個時間點的統計值，例如最大值、最小值、平均值、標準差(使用 groupByKey()與mapValues())"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hYPP0FB6bwo",
        "colab_type": "code",
        "outputId": "f771b631-e3c2-4293-9f48-21c6d099488e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "hour_stat_list = byHourkey.groupByKey().mapValues(list).collect()\n",
        "\n",
        "for i in sorted(hour_stat_list):\n",
        "    print (i[0],max(i[1]),min(i[1]),np.mean(i[1]),np.var(i[1]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 117.0 0.0 26.953424657534246 382.0224882717208\n",
            "1 110.0 0.0 25.704109589041096 362.93710639894914\n",
            "2 107.0 0.0 25.284931506849315 354.74621129667855\n",
            "3 102.0 0.0 23.76164383561644 330.0555151060236\n",
            "4 98.0 0.0 22.865753424657534 310.53814224057044\n",
            "5 88.0 0.0 21.975342465753425 299.8651454306624\n",
            "6 96.0 0.0 22.24931506849315 273.9624995308688\n",
            "7 102.0 0.0 23.572602739726026 286.573495965472\n",
            "8 109.0 0.0 26.246575342465754 311.0241321073372\n",
            "9 114.0 0.0 29.252054794520546 361.73920810658655\n",
            "10 114.0 0.0 29.915068493150685 394.8064852692814\n",
            "11 117.0 0.0 30.265753424657536 407.89375867892664\n",
            "12 112.0 0.0 31.52054794520548 404.74820791893416\n",
            "13 103.0 0.0 30.65205479452055 385.36112591480577\n",
            "14 94.0 0.0 27.86849315068493 346.4484593732408\n",
            "15 104.0 0.0 28.147945205479452 354.9753724901482\n",
            "16 93.0 0.0 28.482191780821918 297.4003677988365\n",
            "17 90.0 0.0 28.295890410958904 303.5343666729217\n",
            "18 88.0 0.0 28.482191780821918 299.4825595796585\n",
            "19 99.0 0.0 29.315068493150687 342.48429348845934\n",
            "20 101.0 0.0 30.136986301369863 365.7127416025521\n",
            "21 100.0 0.0 29.783561643835615 375.3093188215425\n",
            "22 107.0 0.0 29.161643835616438 390.53003565396887\n",
            "23 110.0 0.0 27.852054794520548 368.1315368737099\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AF0Vd9yZ6bwq",
        "colab_type": "text"
      },
      "source": [
        "# 練習2: 請求取2015年，全國pm2.5最高的前十個工作站測點以及其日期。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJBYpK6G6bws",
        "colab_type": "code",
        "outputId": "949548e5-7a63-4dbb-da8a-a63c6e6eeb57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#資料清洗及抓取全國PM2.5資料\n",
        "weatherParse = weatherParse.map(strip_symbol)\n",
        "weatherParse = weatherParse.filter(lambda x: x[2]== \"PM2.5\")\n",
        "weatherParse = weatherParse.map(over)\n",
        "print(weatherParse.first())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['2015/01/01', '龍潭', 'PM2.5', '46', '71', '76', '74', '65', '62', '56', '50', '52', '56', '54', '47', '40', '36', '37', '27', '30', '25', '26', '24', '18', '16', '11', '14']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sEW-ZIo6bww",
        "colab_type": "code",
        "outputId": "e71207e6-4c3d-42c9-f439-3b96f05ce7f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "def hour_key_pair_generation(x):\n",
        "    hour_key_pair = []\n",
        "    for i in range(3,27):\n",
        "        hour_key_pair.append((x[0], x[1],float(x[i])))\n",
        "    return hour_key_pair\n",
        "\n",
        "hour_key_pair_wea_pm25 = weatherParse.map(hour_key_pair_generation)\n",
        "\n",
        "for i in hour_key_pair_wea_pm25.first():\n",
        "    print (i)\n",
        "#print(hour_key_pair_wea_pm25.take(50))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('2015/01/01', '龍潭', 46.0)\n",
            "('2015/01/01', '龍潭', 71.0)\n",
            "('2015/01/01', '龍潭', 76.0)\n",
            "('2015/01/01', '龍潭', 74.0)\n",
            "('2015/01/01', '龍潭', 65.0)\n",
            "('2015/01/01', '龍潭', 62.0)\n",
            "('2015/01/01', '龍潭', 56.0)\n",
            "('2015/01/01', '龍潭', 50.0)\n",
            "('2015/01/01', '龍潭', 52.0)\n",
            "('2015/01/01', '龍潭', 56.0)\n",
            "('2015/01/01', '龍潭', 54.0)\n",
            "('2015/01/01', '龍潭', 47.0)\n",
            "('2015/01/01', '龍潭', 40.0)\n",
            "('2015/01/01', '龍潭', 36.0)\n",
            "('2015/01/01', '龍潭', 37.0)\n",
            "('2015/01/01', '龍潭', 27.0)\n",
            "('2015/01/01', '龍潭', 30.0)\n",
            "('2015/01/01', '龍潭', 25.0)\n",
            "('2015/01/01', '龍潭', 26.0)\n",
            "('2015/01/01', '龍潭', 24.0)\n",
            "('2015/01/01', '龍潭', 18.0)\n",
            "('2015/01/01', '龍潭', 16.0)\n",
            "('2015/01/01', '龍潭', 11.0)\n",
            "('2015/01/01', '龍潭', 14.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3Ahgkm-6bwy",
        "colab_type": "code",
        "outputId": "a44ed3fe-3701-454b-a1a9-915987dee941",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "final = weatherParse\\\n",
        "  .flatMap(hour_key_pair_generation)\\\n",
        "  .map(lambda x: (x[2],x[0],x[1])).top(10)\n",
        "\n",
        "for i in final:\n",
        "  print (i[0], i[1], i[2] )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "478.0 2015/08/29 安南\n",
            "477.0 2015/08/29 安南\n",
            "476.0 2015/06/23 埔里\n",
            "467.0 2015/06/24 埔里\n",
            "459.0 2015/09/29 臺西\n",
            "454.0 2015/06/23 埔里\n",
            "450.0 2015/06/23 埔里\n",
            "444.0 2015/05/11 竹山\n",
            "420.0 2015/12/29 板橋\n",
            "408.0 2015/08/08 古亭\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAYJncl_6bwz",
        "colab_type": "text"
      },
      "source": [
        "Plot your results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JuqkVh376bwz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWayHXWE6bw2",
        "colab_type": "text"
      },
      "source": [
        "# 練習3: 請算算看2015全國哪個測站，紫爆天數最多？\n",
        "### 假設當日平均值大於60，則算該日該地區紫爆\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Liq1IcEX6bw2",
        "colab_type": "code",
        "outputId": "3b53ac3d-6775-4f31-ff7e-d67860044b40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def day_avg(x):\n",
        "  pm25_num = x[3:]\n",
        "  avg = []\n",
        "  for i in range(len(pm25_num)):\n",
        "    pm25_num[i] = float(pm25_num[i])\n",
        "  dayavg = sum(pm25_num)/24\n",
        "  avg.append((x[0]))\n",
        "  avg.append((x[1]))\n",
        "  avg.append((x[2]))\n",
        "  avg.append((dayavg))\n",
        "  return avg\n",
        "a = weatherParse.take(1)\n",
        "print(a)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['2015/01/01', '龍潭', 'PM2.5', '46', '71', '76', '74', '65', '62', '56', '50', '52', '56', '54', '47', '40', '36', '37', '27', '30', '25', '26', '24', '18', '16', '11', '14']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4izkMXe6bw5",
        "colab_type": "code",
        "outputId": "9b6d4e3c-61f0-4b7c-f095-ee0c8c4d010c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "All_day_avg = weatherParse.map(day_avg)\n",
        "#print(All_day_avg.first())\n",
        "def avg_over60(x):\n",
        "  if(x[3]>60):\n",
        "    x[3] = 1\n",
        "  else:\n",
        "    x[3] = 0\n",
        "  return x\n",
        "Over60 = All_day_avg.map(avg_over60)\n",
        "print(Over60.take(10))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['2015/01/01', '龍潭', 'PM2.5', 0], ['2015/01/02', '龍潭', 'PM2.5', 0], ['2015/01/03', '龍潭', 'PM2.5', 0], ['2015/01/04', '龍潭', 'PM2.5', 0], ['2015/01/05', '龍潭', 'PM2.5', 0], ['2015/01/06', '龍潭', 'PM2.5', 0], ['2015/01/07', '龍潭', 'PM2.5', 0], ['2015/01/08', '龍潭', 'PM2.5', 0], ['2015/01/09', '龍潭', 'PM2.5', 0], ['2015/01/10', '龍潭', 'PM2.5', 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqLGwS3DS2NS",
        "colab_type": "code",
        "outputId": "b0eb1083-dc01-4524-a9c0-c67bf639a6b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "def location_value(x):\n",
        "  location_value = []\n",
        "  location_value.append(x[1])\n",
        "  location_value.append(x[3])\n",
        "  return location_value\n",
        "loc_val = Over60.map(location_value)\n",
        "#print(loc_val.take(10))\n",
        "loc_val.reduceByKey(lambda x,y: x+y).map(lambda x:(x[1],x[0])).top(10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(28, '竹山'),\n",
              " (28, '斗六'),\n",
              " (25, '崙背'),\n",
              " (21, '金門'),\n",
              " (20, '善化'),\n",
              " (19, '埔里'),\n",
              " (17, '復興'),\n",
              " (17, '左營'),\n",
              " (16, '橋頭'),\n",
              " (16, '小港')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    }
  ]
}